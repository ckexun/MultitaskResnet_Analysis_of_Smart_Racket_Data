{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c083831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95380b",
   "metadata": {},
   "source": [
    "### 特徵工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7123982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT(xreal, ximag):    \n",
    "    n = 2\n",
    "    while(n*2 <= len(xreal)):\n",
    "        n *= 2\n",
    "    \n",
    "    p = int(math.log(n, 2))\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        a = i\n",
    "        b = 0\n",
    "        for j in range(0, p):\n",
    "            b = int(b*2 + a%2)\n",
    "            a = a/2\n",
    "        if(b > i):\n",
    "            xreal[i], xreal[b] = xreal[b], xreal[i]\n",
    "            ximag[i], ximag[b] = ximag[b], ximag[i]\n",
    "            \n",
    "    wreal = []\n",
    "    wimag = []\n",
    "        \n",
    "    arg = float(-2 * math.pi / n)\n",
    "    treal = float(math.cos(arg))\n",
    "    timag = float(math.sin(arg))\n",
    "    \n",
    "    wreal.append(float(1.0))\n",
    "    wimag.append(float(0.0))\n",
    "    \n",
    "    for j in range(1, int(n/2)):\n",
    "        wreal.append(wreal[-1] * treal - wimag[-1] * timag)\n",
    "        wimag.append(wreal[-1] * timag + wimag[-1] * treal)\n",
    "        \n",
    "    m = 2\n",
    "    while(m < n + 1):\n",
    "        for k in range(0, n, m):\n",
    "            for j in range(0, int(m/2), 1):\n",
    "                index1 = k + j\n",
    "                index2 = int(index1 + m / 2)\n",
    "                t = int(n * j / m)\n",
    "                treal = wreal[t] * xreal[index2] - wimag[t] * ximag[index2]\n",
    "                timag = wreal[t] * ximag[index2] + wimag[t] * xreal[index2]\n",
    "                ureal = xreal[index1]\n",
    "                uimag = ximag[index1]\n",
    "                xreal[index1] = ureal + treal\n",
    "                ximag[index1] = uimag + timag\n",
    "                xreal[index2] = ureal - treal\n",
    "                ximag[index2] = uimag - timag\n",
    "        m *= 2\n",
    "        \n",
    "    return n, xreal, ximag   \n",
    "    \n",
    "def FFT_data(input_data, swinging_times):   \n",
    "    txtlength = swinging_times[-1] - swinging_times[0]\n",
    "    a_mean = [0] * txtlength\n",
    "    g_mean = [0] * txtlength\n",
    "       \n",
    "    for num in range(len(swinging_times)-1):\n",
    "        a = []\n",
    "        g = []\n",
    "        for swing in range(swinging_times[num], swinging_times[num+1]):\n",
    "            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))\n",
    "            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))\n",
    "\n",
    "        a_mean[num] = (sum(a) / len(a))\n",
    "        \"\"\"  Modify: 將sum(a)/len(g)改為g，但結果好像沒有比較好，要再觀察  \"\"\"\n",
    "        g_mean[num] = (sum(g) / len(g))\n",
    "    \n",
    "    return a_mean, g_mean\n",
    "\n",
    "def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):\n",
    "    allsum = []\n",
    "    mean = []\n",
    "    var = []\n",
    "    rms = []\n",
    "    XYZmean_a = 0\n",
    "    a = []\n",
    "    g = []\n",
    "    a_s1 = 0\n",
    "    a_s2 = 0\n",
    "    g_s1 = 0\n",
    "    g_s2 = 0\n",
    "    a_k1 = 0\n",
    "    a_k2 = 0\n",
    "    g_k1 = 0\n",
    "    g_k2 = 0\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        if i==0:\n",
    "            allsum = input_data[i]\n",
    "            a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
    "            g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
    "            continue\n",
    "        \n",
    "        a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
    "        g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
    "       \n",
    "        allsum = [allsum[feature_index] + input_data[i][feature_index] for feature_index in range(len(input_data[i]))]\n",
    "        \n",
    "    mean = [allsum[feature_index] / len(input_data) for feature_index in range(len(input_data[i]))]\n",
    "    \n",
    "    \"\"\"  Modify: 計算variance時負數開根號的問題  \"\"\"\n",
    "    var = [0] * len(input_data[0])\n",
    "    rms = [0] * len(input_data[0])\n",
    "\n",
    "    # 遍歷每一筆感測資料\n",
    "    for i in range(len(input_data)):\n",
    "        for j in range(len(input_data[i])):\n",
    "            var[j] += (input_data[i][j] - mean[j]) ** 2\n",
    "            rms[j] += input_data[i][j] ** 2\n",
    "        \n",
    "    var = [math.sqrt((var[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
    "    rms = [math.sqrt((rms[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
    "    \n",
    "    a_max = [max(a)]\n",
    "    a_min = [min(a)]\n",
    "    a_mean = [sum(a) / len(a)]\n",
    "    g_max = [max(g)]\n",
    "    g_min = [min(g)]\n",
    "    g_mean = [sum(g) / len(g)]\n",
    "    \n",
    "    a_var = math.sqrt(math.pow((var[0] + var[1] + var[2]), 2))\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        a_s1 = a_s1 + math.pow((a[i] - a_mean[0]), 4)\n",
    "        a_s2 = a_s2 + math.pow((a[i] - a_mean[0]), 2)\n",
    "        g_s1 = g_s1 + math.pow((g[i] - g_mean[0]), 4)\n",
    "        g_s2 = g_s2 + math.pow((g[i] - g_mean[0]), 2)\n",
    "        a_k1 = a_k1 + math.pow((a[i] - a_mean[0]), 3)\n",
    "        g_k1 = g_k1 + math.pow((g[i] - g_mean[0]), 3)\n",
    "    \n",
    "    a_s1 = a_s1 / len(input_data)\n",
    "    a_s2 = a_s2 / len(input_data)\n",
    "    g_s1 = g_s1 / len(input_data)\n",
    "    g_s2 = g_s2 / len(input_data)\n",
    "    a_k2 = math.pow(a_s2, 1.5)\n",
    "    g_k2 = math.pow(g_s2, 1.5)\n",
    "    a_s2 = a_s2 * a_s2\n",
    "    g_s2 = g_s2 * g_s2\n",
    "    \n",
    "    a_kurtosis = [a_s1 / a_s2]\n",
    "    g_kurtosis = [g_s1 / g_s2]\n",
    "    a_skewness = [a_k1 / a_k2]\n",
    "    g_skewness = [g_k1 / g_k2]\n",
    "    \n",
    "    a_fft_mean = 0\n",
    "    g_fft_mean = 0\n",
    "    cut = int(n_fft / swinging_times)\n",
    "    a_psd = []\n",
    "    g_psd = []\n",
    "    entropy_a = []\n",
    "    entropy_g = []\n",
    "    e1 = []\n",
    "    e3 = []\n",
    "    e2 = 0\n",
    "    e4 = 0\n",
    "    \n",
    "    for i in range(cut * swinging_now, cut * (swinging_now + 1)):\n",
    "        a_fft_mean += a_fft[i]\n",
    "        g_fft_mean += g_fft[i]\n",
    "        a_psd.append(math.pow(a_fft[i], 2) + math.pow(a_fft_imag[i], 2))\n",
    "        g_psd.append(math.pow(g_fft[i], 2) + math.pow(g_fft_imag[i], 2))\n",
    "        e1.append(math.pow(a_psd[-1], 0.5))\n",
    "        e3.append(math.pow(g_psd[-1], 0.5))\n",
    "        \n",
    "    a_fft_mean = a_fft_mean / cut\n",
    "    g_fft_mean = g_fft_mean / cut\n",
    "    \n",
    "    a_psd_mean = sum(a_psd) / len(a_psd)\n",
    "    g_psd_mean = sum(g_psd) / len(g_psd)\n",
    "    \n",
    "    for i in range(cut):\n",
    "        e2 += math.pow(a_psd[i], 0.5)\n",
    "        e4 += math.pow(g_psd[i], 0.5)\n",
    "    \n",
    "    for i in range(cut):\n",
    "        entropy_a.append((e1[i] / e2) * math.log(e1[i] / e2))\n",
    "        entropy_g.append((e3[i] / e4) * math.log(e3[i] / e4))\n",
    "    \n",
    "    a_entropy_mean = sum(entropy_a) / len(entropy_a)\n",
    "    g_entropy_mean = sum(entropy_g) / len(entropy_g)       \n",
    "        \n",
    "    \n",
    "    output = mean + var + rms + a_max + a_mean + a_min + g_max + g_mean + g_min + [a_fft_mean] + [g_fft_mean] + [a_psd_mean] + [g_psd_mean] + a_kurtosis + g_kurtosis + a_skewness + g_skewness + [a_entropy_mean] + [g_entropy_mean]\n",
    "    writer.writerow(output)\n",
    "\n",
    "def data_generate(datapath: str, tar_dir: str):\n",
    "    # datapath = './dataset/39_Test_Dataset/test_data'\n",
    "    # tar_dir = './dataset/39_Test_Dataset/tabular_data_test'\n",
    "    pathlist_txt = Path(datapath).glob('**/*.txt')\n",
    "    os.makedirs(tar_dir, exist_ok=True)\n",
    "    \n",
    "    for file in pathlist_txt:\n",
    "        f = open(file)\n",
    "\n",
    "        All_data = []\n",
    "\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            if line == '\\n' or count == 0:\n",
    "                count += 1\n",
    "                continue\n",
    "            num = line.split(' ')\n",
    "            if len(num) > 5:\n",
    "                tmp_list = []\n",
    "                for i in range(6):\n",
    "                    tmp_list.append(int(num[i]))\n",
    "                All_data.append(tmp_list)\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "        swing_index = np.linspace(0, len(All_data), 28, dtype = int)\n",
    "        # filename.append(int(Path(file).stem))\n",
    "        # all_swing.append([swing_index])\n",
    "\n",
    "        headerList = ['ax_mean', 'ay_mean', 'az_mean', 'gx_mean', 'gy_mean', 'gz_mean', 'ax_var', 'ay_var', 'az_var', 'gx_var', 'gy_var', 'gz_var', 'ax_rms', 'ay_rms', 'az_rms', 'gx_rms', 'gy_rms', 'gz_rms', 'a_max', 'a_mean', 'a_min', 'g_max', 'g_mean', 'g_min', 'a_fft', 'g_fft', 'a_psd', 'g_psd', 'a_kurt', 'g_kurt', 'a_skewn', 'g_skewn', 'a_entropy', 'g_entropy']                \n",
    "        \n",
    "\n",
    "        output_path = Path(tar_dir) / f\"{Path(file).stem}.csv\"\n",
    "        with open(output_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(headerList)\n",
    "\n",
    "            try:\n",
    "                a_fft, g_fft = FFT_data(All_data, swing_index)\n",
    "                a_fft_imag = [0] * len(a_fft)\n",
    "                g_fft_imag = [0] * len(g_fft)\n",
    "                n_fft, a_fft, a_fft_imag = FFT(a_fft, a_fft_imag)\n",
    "                n_fft, g_fft, g_fft_imag = FFT(g_fft, g_fft_imag)\n",
    "                for i in range(len(swing_index)):\n",
    "                    if i==0:\n",
    "                        continue\n",
    "                    feature(All_data[swing_index[i-1]: swing_index[i]], i - 1, len(swing_index) - 1, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer)\n",
    "            except:\n",
    "                print(Path(file).stem)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ea043e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m datapath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/39_Test_Dataset/test_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m tar_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/39_Test_Dataset/tabular_data_test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdata_generate\u001b[49m(datapath, tar_dir)\n\u001b[0;32m      4\u001b[0m datapath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/39_Training_Dataset/train_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m tar_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/39_Training_Dataset/tabular_data_train\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_generate' is not defined"
     ]
    }
   ],
   "source": [
    "datapath = '../dataset/39_Test_Dataset/test_data'\n",
    "tar_dir = '../dataset/39_Test_Dataset/tabular_data_test'\n",
    "data_generate(datapath, tar_dir)\n",
    "datapath = '../dataset/39_Training_Dataset/train_data'\n",
    "tar_dir = '../dataset/39_Training_Dataset/tabular_data_train'\n",
    "data_generate(datapath, tar_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a9ef1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b8caf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_info_path: str, datapath: str):\n",
    "    # 讀取訓練資訊，根據 player_id 將資料分成 80% 訓練、20% 測試\n",
    "    info = pd.read_csv(train_info_path)\n",
    "    unique_players = info['player_id'].unique()\n",
    "    train_players, test_players = train_test_split(unique_players, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 讀取特徵 CSV 檔（位於 \"./tabular_data_train\"）\n",
    "    datalist = list(Path(datapath).glob('**/*.csv'))\n",
    "    target_mask = ['gender', 'hold racket handed', 'play years', 'level']\n",
    "    \n",
    "    # 根據 test_players 分組資料\n",
    "    x_train = pd.DataFrame()\n",
    "    y_train = pd.DataFrame(columns=target_mask)\n",
    "    x_test = pd.DataFrame()\n",
    "    y_test = pd.DataFrame(columns=target_mask)\n",
    "    \n",
    "    for file in datalist:\n",
    "        # 取得檔案名\n",
    "        unique_id = int(Path(file).stem)\n",
    "        row = info[info['unique_id'] == unique_id]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        # 取的該列資料\n",
    "        player_id = row['player_id'].iloc[0]\n",
    "        data = pd.read_csv(file)\n",
    "        # lable資料*27\n",
    "        target = row[target_mask]\n",
    "        target_repeated = pd.concat([target] * len(data))\n",
    "        if player_id in train_players:\n",
    "            x_train = pd.concat([x_train, data], ignore_index=True)\n",
    "            y_train = pd.concat([y_train, target_repeated], ignore_index=True)\n",
    "        elif player_id in test_players:\n",
    "            x_test = pd.concat([x_test, data], ignore_index=True)\n",
    "            y_test = pd.concat([y_test, target_repeated], ignore_index=True)\n",
    "    \n",
    "    # 標準化特徵\n",
    "    scaler = MinMaxScaler() # 將特徵縮放到 [0, 1] 範圍\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(x_train)\n",
    "    X_test_scaled = scaler.transform(x_test)\n",
    "    \n",
    "    group_size = 27\n",
    "\n",
    "    def model_binary(X_train, y_train, X_test, y_test):\n",
    "        clf = RandomForestClassifier(random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        predicted = clf.predict_proba(X_test)\n",
    "        # 取出正類（index 0）的概率\n",
    "        predicted = [predicted[i][0] for i in range(len(predicted))]\n",
    "        \n",
    "        \n",
    "        num_groups = len(predicted) // group_size \n",
    "        if sum(predicted[:group_size]) / group_size > 0.5:\n",
    "            y_pred = [max(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n",
    "        else:\n",
    "            y_pred = [min(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n",
    "        \n",
    "        y_pred  = [1 - x for x in y_pred]\n",
    "        y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n",
    "        \n",
    "        auc_score = roc_auc_score(y_test_agg, y_pred, average='micro')\n",
    "        print(auc_score)\n",
    "\n",
    "        return clf\n",
    "\n",
    "    # 定義多類別分類評分函數 (例如 play years、level)\n",
    "    def model_multiary(X_train, y_train, X_test, y_test):\n",
    "        clf = RandomForestClassifier(random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        predicted = clf.predict_proba(X_test)\n",
    "        num_groups = len(predicted) // group_size\n",
    "        y_pred = []\n",
    "        for i in range(num_groups):\n",
    "            group_pred = predicted[i*group_size: (i+1)*group_size]\n",
    "            num_classes = len(np.unique(y_train))\n",
    "            # 對每個類別計算該組內的總機率\n",
    "            class_sums = [sum([group_pred[k][j] for k in range(group_size)]) for j in range(num_classes)]\n",
    "            chosen_class = np.argmax(class_sums)\n",
    "            candidate_probs = [group_pred[k][chosen_class] for k in range(group_size)]\n",
    "            best_instance = np.argmax(candidate_probs)\n",
    "            y_pred.append(group_pred[best_instance])\n",
    "        \n",
    "        y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n",
    "        auc_score = roc_auc_score(y_test_agg, y_pred, average='micro', multi_class='ovr')\n",
    "        print('Multiary AUC:', auc_score)\n",
    "\n",
    "        return clf\n",
    "\n",
    "    # 評分：針對各目標進行模型訓練與評分\n",
    "    y_train_le_gender = le.fit_transform(y_train['gender'])\n",
    "    y_test_le_gender = le.transform(y_test['gender'])\n",
    "    clf_gender = model_binary(X_train_scaled, y_train_le_gender, X_test_scaled, y_test_le_gender)\n",
    "    \n",
    "    y_train_le_hold = le.fit_transform(y_train['hold racket handed'])\n",
    "    y_test_le_hold = le.transform(y_test['hold racket handed'])\n",
    "    clf_hold = model_binary(X_train_scaled, y_train_le_hold, X_test_scaled, y_test_le_hold)\n",
    "    \n",
    "    y_train_le_years = le.fit_transform(y_train['play years'])\n",
    "    y_test_le_years = le.transform(y_test['play years'])\n",
    "    clf_years = model_multiary(X_train_scaled, y_train_le_years, X_test_scaled, y_test_le_years)\n",
    "    \n",
    "    y_train_le_level = le.fit_transform(y_train['level'])\n",
    "    y_test_le_level = le.transform(y_test['level'])\n",
    "    clf_level = model_multiary(X_train_scaled, y_train_le_level, X_test_scaled, y_test_le_level)\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    joblib.dump(scaler, \"models/scaler.pkl\")\n",
    "    joblib.dump(clf_gender, \"models/gender_model.pkl\")\n",
    "    joblib.dump(clf_hold,   \"models/hold_model.pkl\")\n",
    "    joblib.dump(clf_years,  \"models/years_model.pkl\")\n",
    "    joblib.dump(clf_level,  \"models/level_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "098b5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8116557137641476\n",
      "0.9991449783116451\n",
      "Multiary AUC: 0.6654614151157799\n",
      "Multiary AUC: 0.8229106167352678\n"
     ]
    }
   ],
   "source": [
    "train_info_path = '../dataset/39_Training_Dataset/train_info.csv'\n",
    "datapath = '../dataset/39_Training_Dataset/tabular_data_train'\n",
    "train(train_info_path, datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e98062",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49e87666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_dir: str, test_data_dir: str, submission_path: str):\n",
    "    model_dir = Path(model_dir)\n",
    "    test_data_dir = Path(test_data_dir)\n",
    "\n",
    "    Path(submission_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 收集所有測試檔案（每個選手一份）\n",
    "    datalist = list(test_data_dir.glob('**/*.csv'))\n",
    "    unique_ids = [int(f.stem) for f in datalist]\n",
    "\n",
    "\n",
    "    # 載入所有測試資料\n",
    "    x_test = pd.DataFrame()\n",
    "    unique_ids = []\n",
    "    group_size = 27\n",
    "    for file in datalist:\n",
    "        df = pd.read_csv(file)\n",
    "        x_test = pd.concat([x_test, df], ignore_index=True)\n",
    "        n_groups = len(df) // group_size\n",
    "        unique_ids.extend([int(file.stem)] * n_groups)\n",
    "\n",
    "    scaler = joblib.load(model_dir / \"scaler.pkl\")\n",
    "    X_test_scaled = scaler.transform(x_test)\n",
    "    num_groups = len(X_test_scaled) // group_size\n",
    "\n",
    "    # 每位選手 27 筆資料\n",
    "    group_size = 27\n",
    "    num_groups = len(X_test_scaled) // group_size\n",
    "\n",
    "    # 載入模型\n",
    "    model_gender = joblib.load(model_dir / \"gender_model.pkl\")\n",
    "    model_hold = joblib.load(model_dir / \"hold_model.pkl\")\n",
    "    model_years = joblib.load(model_dir / \"years_model.pkl\")\n",
    "    model_level = joblib.load(model_dir / \"level_model.pkl\")\n",
    "\n",
    "    # 預測 group-wise 機率平均\n",
    "    def predict_groupwise_proba(model, X):\n",
    "        pred_proba = model.predict_proba(X)\n",
    "        results = []\n",
    "        for i in range(num_groups):\n",
    "            group = pred_proba[i*group_size:(i+1)*group_size]\n",
    "            avg_proba = np.mean(group, axis=0)\n",
    "            results.append(avg_proba)\n",
    "        return np.array(results)\n",
    "    \n",
    "    def predict_binary_groupwise(model, X):\n",
    "        pred_proba = model.predict_proba(X)\n",
    "        results = []\n",
    "        for i in range(num_groups):\n",
    "            group = pred_proba[i*group_size:(i+1)*group_size, 0]  # 類別 0 概率（male 或 right）\n",
    "            if np.mean(group) > 0.5:\n",
    "                result = max(group)  # 選擇最大概率\n",
    "            else:\n",
    "                result = min(group)  # 選擇最小概率\n",
    "            results.append([result, 1 - result])  # [P(class_0), P(class_1)]\n",
    "        return np.array(results)\n",
    "    \n",
    "    def predict_multiary_groupwise(model, X):\n",
    "        pred_proba = model.predict_proba(X)\n",
    "        results = []\n",
    "        for i in range(num_groups):\n",
    "            group = pred_proba[i*group_size:(i+1)*group_size]\n",
    "            class_sums = np.sum(group, axis=0)\n",
    "            chosen_class = np.argmax(class_sums)\n",
    "            candidate_probs = group[:, chosen_class]\n",
    "            best_instance = np.argmax(candidate_probs)\n",
    "            results.append(group[best_instance])\n",
    "        return np.array(results)\n",
    "\n",
    "    # 執行四個任務預測\n",
    "    gender_probs = predict_groupwise_proba(model_gender, X_test_scaled)    \n",
    "    # print(gender_probs)\n",
    "    # print(predict_binary_groupwise(model_gender, X_test_scaled))\n",
    "    hold_probs   = predict_groupwise_proba(model_hold, X_test_scaled)\n",
    "\n",
    "    years_probs  = predict_multiary_groupwise(model_years, X_test_scaled)\n",
    "    level_probs  = predict_multiary_groupwise(model_level, X_test_scaled)\n",
    "\n",
    "    # 建立 submission dataframe\n",
    "    submission = pd.DataFrame({\n",
    "        'unique_id': unique_ids,\n",
    "        'gender': np.round(gender_probs[:, 0], 6),\n",
    "        'hold racket handed': np.round(hold_probs[:, 0], 6),\n",
    "        'play years_0': np.round(years_probs[:, 0], 6),\n",
    "        'play years_1': np.round(years_probs[:, 1], 6),\n",
    "        'play years_2': np.round(years_probs[:, 2], 6),\n",
    "        'level_2': np.round(level_probs[:, 0], 6),\n",
    "        'level_3': np.round(level_probs[:, 1], 6),\n",
    "        'level_4': np.round(level_probs[:, 2], 6),\n",
    "        'level_5': np.round(level_probs[:, 3], 6)\n",
    "    })\n",
    "    submission = submission.sort_values(by=\"unique_id\").reset_index(drop=True)\n",
    "    submission.to_csv(submission_path, index=False, encoding='utf-8', lineterminator='\\n')\n",
    "    print(f\"Submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e92e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to ./result/submission0426.csv\n"
     ]
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "model_dir = \"models\"\n",
    "test_data_dir = \"../dataset/39_Test_Dataset/tabular_data_test\"\n",
    "submission_path = f\"./result/submission{today.strftime('%m%d')}.csv\"\n",
    "inference(model_dir, test_data_dir, submission_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
